Facial Emotion Recognition Using CNN

This repository presents a facial emotion recognition system developed with Convolutional Neural Networks (CNNs) and trained on the FER-2013 dataset. The model predicts emotional states from facial images, supporting use cases in sentiment analysis, human–computer interaction, intelligent systems, and social robotics.

Project Overview

Facial emotion recognition is an important task in computer vision, contributing to fields such as behavioral research, interactive applications, and automated social analysis. In this project, a CNN-based model is designed and trained to interpret human emotions from facial expressions. The FER-2013 dataset serves as the foundation for training and evaluation, offering a reliable benchmark commonly used for emotion detection research.

Key Features
Dataset

Utilizes the FER-2013 dataset containing 35,887 grayscale facial images.

Each image is 48×48 pixels and categorized into seven emotion classes:
Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral.

Model

A custom-built CNN architecture tailored for multi-class emotion classification.

Designed to capture spatial features and patterns in facial expressions.

Evaluation

Performance is assessed using accuracy, F1-score, and a confusion matrix to ensure balanced and reliable results.

Comprehensive visualization tools are included to examine learning behavior and prediction quality.

Training

Multiple experiments with tuning hyperparameters to enhance generalization and improve model robustness.

Training and validation metrics are plotted for deeper performance insight.

Visualization

Includes plots for training/validation accuracy and loss.

Confusion matrix heatmap to clearly represent classification strengths and weaknesses.
